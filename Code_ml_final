# import necessary libs
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
import statsmodels.formula.api as smf
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from numpy import nan
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import BaggingClassifier


# data import and overview
import_data = "https://github.com/EnergizedTechLabsDo/energized/raw/master/All_Data_final.xlsx"
df = pd.read_excel(import_data)
print(df)
df["Entity"].drop_duplicates()
# df = df.set_index("Entity")

print(df.shape)
print(len(df))
print(df.describe())
print(df.loc[:, df.isnull().any()])
print(df.isnull().sum())
print(df.isnull().sum())
print(df.head(-20))

df.drop(["Entity"], axis=1)
fig, ax = plt.subplots(figsize=(10, 7))
res = sns.heatmap(df.corr(method="pearson"), cmap="RdBu", center=0, annot=True)
res.set_xticklabels(res.get_xticklabels(), rotation=17,
                    horizontalalignment='right', fontsize='medium')
res.set_yticklabels(res.get_yticklabels(), rotation=0,
                    horizontalalignment='right', fontsize='medium')
# res.invert_yaxis()
#res.set_xticklabels(res.get_xmajorticklabels(), fontsize=5)
#res.set_yticklabels(res.get_ymajorticklabels(), fontsize=8)
ax.set_title("Multi-Collinearity of Energy Attributes", fontsize=15)
plt.show()


# Supervised Maschine Learning Content Overview of data/preprocessing target and feature
X = df["Energy use per capita"].values.reshape(-1, 1)
y = df["CO2e per capita"].values.reshape(-1, 1)
print("Dimensions of y before reshaping: {}".format(df["CO2e per capita"].shape))
print("Dimensions of X before reshaping: {}".format(df["Energy use per capita"].shape))
print("Dimensions of y after reshaping: {}".format(y.shape))
print("Dimensions of x after reshaping: {}".format(X.shape))
print(X)
# futsch
sns.set_style("whitegrid")
fig, ax = plt.subplots(2)
ax[0] = sns.JointGrid(X, y)
ax[0].plot(sns.regplot, sns.distplot)
plt.xlabel("Energy use [kWh/capita]", fontsize=12)
plt.ylabel("CO2e [t/capita]", fontsize=12)
plt.title("Visualizing the Correlation", fontsize=15)
plt.tight_layout()
plt.show()
plt.clf()


SEED = 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)
model = LinearRegression()
model.fit(X_train, y_train)
prediction_space = np.linspace(min(X), max(X)).reshape(-1, 1)
y_pred_rmse = model.predict(X_test)
y_pred = model.predict(prediction_space)
rmse_lin = mean_squared_error(y_test, y_pred_rmse)**(1/2)
score_linear = model.score(X_test, y_test)
print("\nScore of model: ", score_linear)
plt.xlabel("Energy use [kWh/capita]", fontsize=12)
plt.ylabel("CO2e [t/capita]", fontsize=12)
plt.scatter(X, y, color="red")
correlation, pvalue = pearsonr(df["Energy use per capita"], df["CO2e per capita"])
plt.plot(prediction_space, y_pred, color="black", linewidth=4)
plt.show()
print("\nCorrelation of model: ", correlation)


# initiate supervised maschine learning model Regression and get models performance
SEED = 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)
steps = [("StandardScaler", StandardScaler()), ("RidgeReg", Ridge(alpha=1.0, copy_X=True, fit_intercept=True,
                                                                  max_iter=None, normalize=False, random_state=None,
                                                                  solver='auto', tol=0.001))]
pipeline = Pipeline(steps)
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
rmse_rid1 = mean_squared_error(y_test, y_pred)
score_model = pipeline.score(X_test, y_test)
print("\nScore of model RID1: ", score_model)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error of RID1: {}".format(rmse_rid1))

# Finetune model and look for best options
SEED = 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)
ridge = Ridge()
param_grid = {"alpha": np.arange(start=0.1, stop=1.1, step=0.1)}
ridge_cv = GridSearchCV(ridge, param_grid, cv=10)
ridge_cv.fit(X_train, y_train)
y_pred_ridge_new = ridge_cv.predict(X_test)
rmse_rid = mean_squared_error(y_test, y_pred_ridge_new)**(1/2)
print("Tuned Ridgereg RMSE: {}".format(rmse_rid))
print("\n best parameter to choose: ", ridge_cv.best_params_)
print("\n best score:", ridge_cv.score(X_test, y_test))
score_ridge = ridge_cv.score(X_test, y_test)

# test elastic net
SEED = 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)
l1_space = np.linspace(0, 1, 30)
param_grid = {'l1_ratio': l1_space}
elastic_net = ElasticNet()
gm_cv = GridSearchCV(elastic_net, param_grid, cv=10)
gm_cv.fit(X_train, y_train)
y_pred_elastic = gm_cv.predict(X_test)
score_elastic = gm_cv.score(X_test, y_test)
rmse_elas = mean_squared_error(y_test, y_pred_elastic)**(1/2)
print("Tuned ElasticNet l1 ratio: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet score: {}".format(score_elastic))
print("Tuned ElasticNet RMSE: {}".format(rmse_elas))

# non-linear model svm
SEED = 1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)
steps = [("StandardScaler", StandardScaler()), ("SVM", SVR())]
pipeline_new = Pipeline(steps)
pipeline_new.fit(X_train, np.ravel(y_train, order='C'))
y_pred_svm = pipeline_new.predict(X_test)
score_svm = pipeline_new.score(X_test, y_test)
rmse_svm = mean_squared_error(y_test, y_pred_svm)**(1/2)
print("SVM score: {}".format(score_svm))
print("SVM MSE: {}".format(rmse_svm))

# RandomForestRegressor with Bootstrapping
x = df["Energy use per capita"].values.reshape(-1, 1)
y = df["CO2e per capita"].values.reshape(-1, 1)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)
rf = RandomForestRegressor(random_state=42, n_estimators=255,
                           min_samples_leaf=6, max_leaf_nodes=25, max_depth=30)
rf.fit(x_train, y_train)
y_pred = rf.predict(x_test)
score_rf = rf.score(x_test, y_test)
rmse_rf = mean_squared_error(y_test, y_pred)**(1/2)
print("\nScore of model: ", score_rf)
print("Root Mean Squared Error: {}".format(rmse_rf))


# plot best predict
xt = X_test[:20]
pred_linear = model.predict(xt)
pred_ridge = pipeline.predict(xt)
pred_elastic = gm_cv.predict(xt)
pred_svm = pipeline_new.predict(xt)
pred_rf = rf.predict(xt)
plt.plot(pred_elastic, 'r*', ms=10, label='ElasticRegression')
plt.plot(pred_linear, 'ys', label='LinearRegression')
plt.plot(pred_ridge, 'b^', label='RidgeRegression')
plt.plot(pred_svm, 'D', label='SVMRegression')
plt.plot(pred_rf, "P", label="RandomForestRegression")
plt.tick_params(axis='x', which='both', bottom=False, top=False,
                labelbottom=False)
plt.ylabel('predicted', fontsize=12)
plt.xlabel('training samples', fontsize=12)
plt.legend(loc="best")
plt.title('Regressor predictions and their average', fontsize=15)
plt.show()

plt.plot(score_linear, 'ys', label="LinearRegression")
plt.plot(score_ridge, 'b^', label="'RidgeRegression'")
plt.plot(score_elastic, 'r*', label="ElasticRegression'")
plt.plot(score_svm,  'D', label="SVMRegression")
plt.plot(score_rf, "P", label="RandomForestRegression")
plt.tick_params(axis='x', which='both', bottom=False, top=False,
                labelbottom=False)
plt.ylabel('Score of Models', fontsize=12)
plt.xlabel('Choose', fontsize=12)
plt.legend(loc="best")
plt.title('Which model predicts the best?', fontsize=15)
plt.show()

# rmse plot
plt.plot(rmse_lin, 'ys', label="LinearRegression")
plt.plot(rmse_rid, 'b^', label="'RidgeRegression'")
plt.plot(rmse_elas, 'r*', label="ElasticRegression'")
plt.plot(rmse_svm,  'D', label="SVMRegression")
plt.plot(rmse_rf, "P", label="RandomForestRegression")
plt.tick_params(axis='x', which='both', bottom=False, top=False,
                labelbottom=False)
plt.ylabel('RMSE of Models', fontsize=12)
plt.xlabel('Choose', fontsize=12)
plt.legend(loc="best")
plt.title('RMSE in comparison', fontsize=15)
plt.show()
